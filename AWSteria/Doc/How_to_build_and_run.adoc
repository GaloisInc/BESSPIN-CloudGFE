= Building and running AWSteria/Flute/Virtio
Rishiyur S. Nikhil, Bluespec, Inc. (c) 2020
:revnumber: v0.1
:revdate: 2020-12-14
:sectnums:
:toc:
:toclevels: 5
:toc: left
:toc-title: Contents
:description: How to build and run AWSteria in simulation and on Amazon AWS F1
:keywords: AWSteria, BSV, Flute, Virtio
:imagesdir: Figures
:data-uri:

// SECTION ================================================================
== Introduction

This document describes how to build and run the host side and the
hardware side for an AWSteria/Flute/Virtio setup, in simulation and on
an actual Amazon AWS F1 Instance (i.e., an AWS server with attached
FPGA).  In both scenarios, there are two "`processes`" involved: a
"`host-side`" and a "`hardware-side`".  The host-side process is
normal Linux process.  In simulation, the hardware-side processs is
also a normal Linux process, a Bluesim or verilator simulation of a
hardware design.  On an AWS F1 instance, the hardware-side is the
design on the attached FPGA.

On Amazon AWS F1::

* This has to run on an AWS F1 instance (i.e., an AWS server with attached FPGA).
* The hardware side is an AFI (Amazon FPGA Image) containing a bitfile that is loaded into the FPGA.
* The host side is a Linux executable
* The two sides communicate over AWS-provided code infrastructure,
    which provides AXI4 and AXI4-Lite abstractions.  On the host-side,
    these are an API for reads, writes and DMA transfers.  On the hardware-side, these are
    actual AXI4 ("`DMA PCIS`") and AXI4-Lite ("`OCP`") interfaces that connect to your design.
    The communication is carried over a hardware PCIe interface.

Simulation::

* Simulation is typically used for initial debugging of a design, and
   can be done on any computer (does not have to be on Amazon AWS).
* The hardware side is a Bluesim or a verilator simulation running as a Linux executable.
    ** Note: what we describe here is different from the AWS-supplied
         example simulation flow, which uses XSIM, Xilinx Vivado's
         simulator.  You will need that flow instead, if your design
         contains IP that needs Vivado licenses, Vivado encrypted IP,
         etc.
* The host side is a Linux executable.
* The two sides communicate over TCP, using a provided emulation of AWS F1's
    PCIe/AXI4-Lite/AXI4 communication.

The host-side user code and hardware-side user design are identical in
the two scenarios.  The only difference is in the build flow.

// SUBSECTION ================================================================
== AWSteria/Flute/Virtio

On the hardware side, AWSteria/Flute/Virtio contains:

* The Bluespec Flute open-source RISC-V CPU, configured with Writeback
    coherent caches (`WB_L1_L2`).  This includes separate L1 caches
    for Instruction and Data and a shared L2 cache

* A Bluespec RISC-V Debug Module for debugging the program on the
    RISC-V CPU from the host side.

* DRAM (uses the AWS F1-provided DRAM)

* A UART for communication with a console on the host side

* Hardware to support "`Virtio`" (more below).

On the host side, AWSteria/Flute/Virtio contains:

* Software to load a RISC-V executable (ELF or Memhex file), download
    the contents into the DRAM in the hardware, and start the RISC-V
    CPU's execution on that code.

* A tty console for the RISC-V CPU.

* A GDB connection to the Debug Module on the RISC-V CPU

* [This feature is not yet implemented] A way to record "`tandem verification`" traces from the RISC-V CPU.

* "`Virtio`" device emulation and support for the RISC-V CPU (more below).

*About Virtio*

Virtio is an open standard for a "`guest`" OS to have access to
devices (particularly network and block storage) provided by a
"`host`" OS (cf.
https://developer.ibm.com/technologies/linux/articles/l-virtio/[]).
AWSteria/Flute/Virtio provides this capability, where the RISC-V CPU runs the
"`guest`" OS and the "`host` OS" is code running on the AWS x86 server
running Linux.  In this way, the RISC-V CPU has virtual access to
network, disk and other devices, even though the AWS F1 FPGA does not
directly connect to any such devices.  AWSteria/Flute/Virtio provides these
services through a combination of host-side software and hardware-side support
infrastructure.

// SECTION ================================================================
== ENVIRONNMENT VARIABLES

The following environment variables are used at various points in the
build-and-run flow.

Location of your clone of the AWS FPGA repository https://github.com/aws/aws-fpga.git[]:
----
    AWS_FPGA_REPO_DIR = ...
----

Location of your clone of the AWSteria repository
https://github.com/DARPA-SSITH-Demonstrators/BESSPIN-CloudGFE[]:
----
    AWSTERIA = ...
----

Location of your clone of the Flute repository https://github.com/bluespec/Flute[]:
----
    FLUTE_REPO = ...
----

After you build your host-side executable, its name (`test_hello_world` or `exe_host_side`):
----
    SW_EXE = exe_host_side
----

// SECTION ================================================================
== Build and Run a Simulation

// SUBSECTION ================================================================
=== Building for Bluesim

Build the Bluesim executable (this requires the `bsc` compiler to be
set up, with environment variables `BLUESPECDIR`, etc.):
----
    $ cd  $(AWSTERIA)/builds/RV64GC_MSU_Flute_AWS_bluesim/
    $ make  all
----

This will invoke `bsc` to compile and build the "`hardware-side`" into
a Bluesim simulator, embodied in two files that together form a Linux
executable (the first is a shell script that simply loads the shared
object in the second file and invokes it):

----
    exe_HW_sim*
    exe_HW_sim.so*
----

Build the host-side software executable:

----
    $ cd  $(AWSTERIA)/src_Host_Side/
    $ make
----

This will build the host-side executable, embodied as an ELF file:
----
    exe_host_side
----

// SUBSECTION ================================================================
=== Run the host-side software and the Bluesim hardware simulation

You will need two terminal windows, call them _T1_ (hardware side) and
_T2_ (host side).

_In terminal window T1 (hardware side):_ start the Bluesim executable as follows. It
will immediately pause, waiting for a TCP connection on socket 30000
from the host-side software.

----
    $ cd  $(AWSTERIA)/builds/RV64GC_MSU_Flute_AWS_bluesim
    $ ./exe_HW_sim
----

_In terminal window T2 (host side):_ start the host-side executable. It will
connect to the Bluesim executable on the TCP socket, and then both
will run concurrently.

----
    $ cd  $(AWSTERIA)/src_Host_Side
    $ ./exe_host_side    <memhex32 file>
----

The argument should be a MemHex file (32-bit-wide) for the program
that you want the RISC-V CPU to execute.

// SECTION ================================================================
== Build for AWS F1

This section shows the steps to create an AFI (Amazon AWS FPGA image)
for AWSteria/Flute/Virtio.  This does not have to be done on an AWS
machine, it can be on your own computer (what Amazon calls "`customer
premises`"), as long as you have the required Xilinx Vivado licenses.

This section repeats information from `README` in the repository
https://github.com/aws/aws-fpga/tree/master/hdk[].  The numeric step
numbers below correspond to step numbers in that reference.  Hopefully
this is a simpler and clearer narrative.

// SUBSECTION ================================================================
=== Overview of the build process

* Your hardware design (which will go on the AWS F1 FPGA) can be designed
    and debugged with any EDA tools, but ultimately must have a
    top-level Verilog module with a specific interface (input and
    output signals and buses) so that it will "`fit`" into the
    AWS-provided environment called the _Shell_.  The Shell provides
    clocks and resets, AXI4 and AXI4-Lite interfaces for communication
    with the host, and AXI4 interfaces for communicating with DDR4
    memory.

* When your design is ready, you will invoke a step that results in a
    "`Design Checkpoint`" (DCP) which you submit to some AWS tools
    which will (in the background) perform Vivado synthesis and
    integration with the AWS shell, resulting in a "`bitfile`" that
    can be loaded into the AWS F1 FPGA.  This submission involves
    uploading files into an AWS "`bucket`" that you create (a bucket
    is a unit in AWS's cloud storage system).

// SUBSUBSECTION ================================================================
=== One-time step: Set up Xilinx Vivado

Earlier, you (or your administrator) should have downloaded and
installed Xilinx Vivado, Xilinx licenses etc.

*Step 0a:* To run Vivado, set it up using the Xilinx-provided script:
----
    $ source  /tools/Xilinx/Vivado/2019.1/settings64.sh
----

*Step 0b:* Several commands to check on your Vivado setup:
----
    $ which vivado
    $ vivado  -version
    $ vivado  -mode batch
----

// SUBSUBSECTION ================================================================
=== One-time step: Set up AWS repo and tools

*Step 0c:* git-clone the AWS-FPGA repository:
----
    $git clone  https://github.com/aws/aws-fpga.git  $(AWS_FPGA_REPO_DIR)
----

[[Setup_HDK]]
*Step 0e:* Set up the Amazon AWS HDK (Hardware Design Kit).  This
defines `HDK_DIR` and other environment variables.  The very first
time you do this after downloading the `aws-fpga` repo, this can take
several minutes, since it invokes Vivado to build ddr4 simulation
models.

----
    $ cd  $(AWS_FPGA_REPO_DIR)
    $ source hdk_setup.sh
----

AWS tools have a capability to send you an email notification on
completion, which is useful for long-running steps.

// SUBSECTION ================================================================
=== Define `CL_DIR` and go there

[[Set_CL_DIR]]

Your DCP builds will be done within particular directories inside your
clone of the `aws-repo` repository. You should set environment
variable `CL_DIR` to point at this directory.

If you were building the AWS-supplied examples, you'd use definitions like this:

----
    CL_DIR ?= $(AWS_FPGA_REPO_DIR)/hdk/cl/examples/cl_hello_world
    CL_DIR ?= $(AWS_FPGA_REPO_DIR)/hdk/cl/examples/cl_dram_dma
----

AWS documentation recommends doing your designs in a directory
`developer_designs` (sibling to `examples`), copying one of their
supplied examples.  We'll use:

----
    CL_DIR ?= $(AWS_FPGA_REPO_DIR)/hdk/cl/developer_designs/cl_BSV_GFE
----

*Step 1a:* Change to that directory:

----
    $ cd  $(CL_DIR)
----

// SUBSECTION ================================================================
=== Build a DCP file (Design Checkpoint)

This step can take a long time (possibly hours) because it does a
Vivado synthesis of your design.  So, it runs in the background, in a
`nohup` environment, so that it continues even if your terminal goes
away.  Its output is captured in a logfile with a name in this format
`yy_mm_dd-hhmmss.nohup.out` so you can follow its progress if you
wish.

You will be running the AWS script
`$(CL_DIR)/build/scripts/aws_build_dcp_from_cl.sh`.  This has a number
of command-line flags, which you can collect in the environment
variables `BUILD_DCP_FLAGS`.

*Clocks*: You can control which of several standard clocks should be
supplied to your design from the AWS shell.  If you don't specify
anything, you get the default "`Clock Group A Recipe A0 (125 MHz)`"

As an example alternative, the following will set it up for "`Clock Group A Recipe A1 (250 MHz)`":
----
    BUILD_DCP_FLAGS += -clock_recipe_a A1
----

[NOTE]
====
Because the build step can take many hours, you may wish to receive an
email from AWS when the step has completed.  This is how you set it up:

*Step 2a:* One-time step: register an email address with AWS for notifications:
----
    EMAIL ?= <your email address for notifications>
    $ $(AWS_FPGA_REPO_DIR)/shared/bin/scripts/notify_via_sns.py
----

When you run this it will send you an email asking for you to click
for confirmation, and pause waiting until you do so.  Note: email
confirmations go through `sns.amazonaws.com`.

`notify_via_sns.py` is a Python program, so of course you will need
Python installed.  It may still fail with error: "`missing python
package 'boto3'`" This can be fixed by adding the `boto3` package in
your Python installation:
----
    $ pip install boto3        # For Python 2.x
    $ pip3 install boto3       # For Python 3.x
----

For the DCP build, add this flag to request a notification:
----
    BUILD_DCP_FLAGS += -notify
----
====




This flag avoids "`ERROR: your instance has less mem than is
necessary`", concerning the machine on which you are running the build:
----
    BUILD_DCP_FLAGS += -ignore_memory_requirement
----

On an Amazon AMI (AWS Machine Instance), if you need to resize it to
have more memory, please see this document;
http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html[].

During the build, you may see many warnings.  You can see examples by
viewing warnings produced while building the AWS-supplied examples,
in:

----
    $(AWS_FPGA_REPO_DIR)/hdk/cl/examples/cl_hello_world/build/scripts/warnings.txt
    $(AWS_FPGA_REPO_DIR)/hdk/cl/examples/cl_dram_dma/build/scripts/warnings.txt
----

Checklist before running `$CL_DIR/build/scripts/aws_build_dcp_from_cl.sh`:

* Environment variable `$HDK_SHELL_DIR` is set (see Section <<Setup_HDK>>)
* Environment variable `$CL_DIR` is set (see Section <<Set_CL_DIR>>)
+
--
This contains sub-directories:
----
    build/
    ├── constraints
    ├── scripts
    └── src_post_encryption
----

During execution, the script will create some more sub-directories:
----
    build/
    ├── checkpoints
    │   └── to_aws
    ├── constraints
    ├── reports
    ├── scripts
    └── src_post_encryption
----
--

* Update the following files for design-specific options:

** List of all source files including header files (.inc, .h, .vh).
    These will be copied to `src_post_encryption/`
    and encrypted there (if you are doing encryption)
    and any other design-specifics.
    Comment-out the last few `encrypt` commands if you're not doing encryption
+
----
    build/scripts/encrypt.tcl
----

** For your design specifics, specifically around IP sources and xdc files,
   and your specific design xdc files.
+
----
    build/scripts/create_dcp_from_cl.tcl
----

** For timing and placement constraints
+
----
    build/constraints/*.xdc
----

*Step 2b:* Then, build your DCP.  This can take a while (for standard
AWS-supplied example Hello World, took it 93 minutes on a AWS
t2.2xlarge AMI machine).
----
    $ cd $(CL_DIR)/build/scripts
    $ ./aws_build_dcp_from_cl.sh  $(BUILD_DCP_FLAGS)
----

When finished, the step produces a file whose name has a timestamped format:
----
    $(CL_DIR)/build/checkpoints/to_aws/YY_MM_DD-hhmm.Developer_CL.tar
----
comprising a DCP file, and other log/manifest files.
In a later step, the DCP file will be submitted to AWS to create an AFI.

*Step 2c:* Check that it created your DCP tarfile, and export an
environment variable `DCP_TARFILE` with the tarfile's name:
----
    $ ls  $(CL_DIR)/build/checkpoints/to_aws/*.Developer_CL.tar
    $  export  DCP_TARFILE=<the DCP tarfile that was just created>
----

// SUBSECTION ================================================================
=== Upload/submit the DCP file to AWS so it can package it as an AFI

// SUBSUBSECTION ================================================================
==== One-time step: Install AWS CLI

Make sure you have the AWS CLI (Command-Line Interface) tools
installed which gives you the `aws` command on the command line.

These are pre-installed in certain AWS AMIs (Amazon Machine Instances)
such as `FPGA Developer AMI-1.8.1`. (When you create an Amazon AWS
instance, you can select this in the menu to specify the kind of
instance you want.)

To install it on other machines or AWS instances, use the
package manager for the machine's OS (like one of the following commands):
----
    $ sudo apt-get install awscli        # Ubuntu or Debian
    $ sudo yum install awscli            # CentOS, ...
    ...                                  # command for your package manager
----

Check that you have the `aws` command installed:
----
    $ which aws
    $ aws  --version
----

[NOTE]
====
When I used the standard AWS AMI `FPGA Developer AMI-1.8.1`,
which has `awscli` pre-installed, and where you think everything
should work out-of-the-box, the commands shown in the next section
failed with Python errors.  In fact all `aws s3` commands failed.  It
seems it has an obsolete version of `awscli`.  I did:
----
    $ sudo pip install awscli
----
to upgrade AWS CLI: it reported:
----
    uninstalled botocore-1.16.7  and installed 1.8.32
    uninstalled s2transfer-0.3.3 and installed s3transfer-0.1.13.
----
after which the 'aws s3' commands started working.
====

// SUBSUBSECTION ================================================================
==== Create an S3 bucket for your submission

Create an S3 "`bucket`" to contain your submission for an AFI build.
S3 is Amazon AWS' cloud storage system.  Buckets are Amazon AWS'
storage units in the cloud; they have globally unique names.

Choose a name for the bucket you create.  Amazon's rules for bucket names:

* 3 to 63 chars long
* lowercase letters, digits, hyphens
          (in particular no underscores; dots allowed but not recommended)
* must start and end with a letter or digit, not hyphen

Example:
----
    DCP_BUCKET ?= rsnbucket1
----

The specific Amazon region that you use:
----
    REGION ?= us-west-2
----

*Step 3a*: Create Bucket
----
    $ aws s3 mb s3://$(DCP_BUCKET) --region $(REGION)
----

*Step 3b:* You can list the buckets visible to you:
----
    $ aws s3 ls
----

// SUBSUBSECTION ================================================================
==== Create a folder in the bucket for the DCP (design checkpoint tarball)

*Step 3c:* Create folder for DCP
----
    $ export DCP_FOLDER = AWSteria    (example)
    $ aws s3 mb s3://$(DCP_BUCKET)/$(DCP_FOLDER)/
----

[NOTE]
====
It seems there are no actual folders on S3. Each bucket is a
unique storage unit with a globally unique name but, for a convenient
'folder view', common prefixes ending in `/` are interpreted by AWS
software and web interface as "`the same hierarchical
directory/folder`".  This is why the command above to create a folder
looks just like the command earlier to create a bucket (they're in
fact the same thing).
====

[NOTE]
====
Step 3c did not work for me (and still does not). I always get this error:
----
     make_bucket failed: s3://$(DCP_BUCKET)/$(DCP_FOLDER)/ An error occurred (BucketAlreadyOwnedByYou)
         when calling the CreateBucket operation: Your previous request to create the named bucket
         succeeded and you already own it.
----
But `aws s3 ls` shows that it does not exist,
and viewing it from the S3 dashboard on the web also shows it does not exist.

The S3 dashboard on the web does have a button to create a
folder, and that worked.
====

// SUBSUBSECTION ================================================================
==== Upload the DCP file to the S3 folder

*Step 3d*: Upload the DCP to the folder:
----
    $ export DCP_TARFILE = 20_05_24-162730.Developer_CL.tar    # (example)
    $ aws s3 cp  $(DCP_TARFILE)  s3://$(DCP_BUCKET)/$(DCP_FOLDER)/
----
Note, the trailing '/' in the command above is is necessary!

// SUBSUBSECTION ================================================================
==== Create a folder where AWS will place logfiles

Create a folder for log files that will be generated during the AFI build,
and move a temporary, empty file LOGS_FILES_GO_HERE.txt there.

*Step 3e*: Create Folder for Logs:
----
    $ export LOGS_BUCKET = $(DCP_BUCKET)
    $ export LOGS_FOLDER = $(DCP_FOLDER)-logs
    $ touch LOGS_FILES_GO_HERE.txt
    $ aws s3 cp LOGS_FILES_GO_HERE.txt s3://$(DCP_BUCKET)/$(LOGS_FOLDER)/
----
Note: the training '/' in the command above is necessary!

This example creates the LOGS_FOLDER "`inside`" the DCP_BUCKET, but I
think that is not necessary; the `create-fpga-image` step below
allows you to specify the bucket and folder for logs separately.
----
    $ aws s3 mb s3://$(DCP_BUCKET)/$(LOGS_FOLDER)/
----

[NOTE]
====
As in the earlier note, the first command (folder-creation) failed
for me, in the same way (`BucketAlreadyOwnedByYou` error).
I ignored it: the next two commands worked.
====

// SUBSUBSECTION ================================================================
==== Submit the DCP to AWS to package it in an AFI

Pick a name and text description for the AFI to be created:
----
    $ export AFI_NAME        = RSNAwsteriaTest3     # (example)
    $ export AFI_DESCRIPTION = "AWSteria take 3"    # (example)
----

AFI creation can be controlled by a number of flags.  You'll
definitely want to use these:
----
    --region $(REGION)
    --name $(AFI_NAME)
    --description $(AFI_DESCRIPTION)
    --input-storage-location Bucket=$(DCP_BUCKET),Key=$(DCP_FOLDER)/$(DCP_TARFILE)
    --logs-storage-location Bucket=$(LOGS_BUCKET),Key=$(LOGS_FOLDER)
----
Other flags of interest:
----
    --client-token <value>    # No idea what this does
    --dry-run
    --no-dry-run
----

Assuming you've defined `CREATE_FPGA_IMAGE_FLAGS` to be your chosen flags,

*Step 3f*: Start AFI Creation
----
    $ aws ec2 create-fpga-image  $(CREATE_FPGA_IMAGE_FLAGS)
----

The command will submit it to the cloud, and immediately print
output that looks like this:
----
    {
        "FpgaImageId": "afi-0fced9721a34d8d99",
        "FpgaImageGlobalId": "agfi-0cb465a4e98968670"
    }
----
You should save this information, for future reference.

AFI ID:: ("`AWS FPGA Image Identifier`") This is the main ID used to manage the AFI
through AWS EC2 CLI commands and AWS SDK APIs. This ID is
regional, i.e., if an AFI is copied across multiple regions,
it will have a different unique AFI ID in each region. An
example AFI ID is afi-06d0ffc989feeea2a.

AGFI ID:: ("`AWS Global FPGA Image Identifier`") This is the global ID
to refer to an AFI from within an F1 instance. E.g.,, to load or clear
an AFI from an FPGA slot, you use the AGFI ID. Since the AGFI IDs is
global (by design), it allows you to copy a combination of AFI/AMI to
multiple regions, and they will work without requiring any extra
setup. An example AGFI ID is agfi-0f0e045f919413242.

Please use these to define environment variables:
----
    $ export AFI_ID  = "afi-0735d1366f0532f90"    # (example)
    $ export AGFI_ID = "agfi-037d7a8803a9d632c"   # (example)
----

// SUBSUBSECTION ================================================================
==== Wait for completion of AFI creation

There are several ways by which you can wait for AWS to complete its
creation of the AFI.

You can study the logs in:
----
    s3://$(LOGS_BUCKET)/$(LOGS_FOLDER)
----

You can run a command-line status check by providing the AFI ID:

*Step 3g*: Check AFI creation_status:
----
    $ aws ec2 describe-fpga-images --fpga-image-ids  $(AFI_ID)
----
This will show a JSON/YAML output; look for a line like:
----
     "State": { "Code" : "pending" or "available" or "failed" }
----

You can request that you get an email notification on AFI build completion:

*Step 3h*: Request AFI completion email notification:
----
    $ wait_for_afi.py --afi $(AFI_ID) --notify --email $(EMAIL)
----

[NOTE]
====
This program is in:
----
    $(AWS_FPGA_REPO_DIR)/shared/bin/scripts/wait_for_afi.py
----
but it should already be in your path due to your HDK setup earlier (see Section <<Setup_HDK>>).
====

// SECTION ================================================================
== Running your design on an AWS F1 Instance

// SUBSECTION ================================================================
=== One-time step: Set up AWS FPGA Management tools

*Step 4a:* Run the following AWS-provided script.  Note, it will prompt
for your password since it does some sub-steps as 'root':

----
    $ cd  $(AWS_FPGA_REPO_DIR)
    $ source sdk_setup.sh
----

// SUBSECTION ================================================================
=== Set up your AWS credentials

*Step 4b:* Configure AWS to set your credentials, in the usual way.
Note: it will prompt for password since it does some sub-steps as
'root':

----
    $ aws configure
----

// SUBSECTION ================================================================
=== Check status and clear any previously loaded AFI

The AWS F1 instance on which you are running may have an AFI (Amazon
FPGA Instance) already loaded in its attached FPGA.  You can check
this as follows:

*Step 5a:* Check status of existing loaded AFI in "`slot 0`", if any:
----
    $ sudo  fpga-describe-local-image  -S 0  -H
----

Your output may look like this (if your instance has an FPGA device):
----
    Type  FpgaImageSlot  FpgaImageId             StatusName    StatusCode   ErrorName    ErrorCode   ShVersion
    AFI          0       none                    cleared           1        ok               0       <shell_version>
    Type  FpgaImageSlot  VendorId    DeviceId    DBDF
    AFIDEVICE    0       0x1d0f      0x1042      0000:00:0f.0
----

*Step 5b:* You can clear (unload) any previously loaded AFI as follows:
----
    $ sudo fpga-clear-local-image  -S 0
----

// SUBSECTION ================================================================
=== Load your new AFI

The environment variable `AGFI_ID` should be defined to name your
desired AFI (given to you after Step 3f).

*Step 5c:* Load your new AFI into Slot 0 (note, using `AGFI_ID`, not `AFI_ID`):
----
    $ sudo fpga-load-local-image -S 0 -I $(AGFI_ID)
----
Note: This optionally takes a flag like `-a 87`
or `-a 97` to run it at 87 MHz or 97 MHz, respectively, which may be
different from the target MHz during synthesis (DCP build). The
following page has more information on setting runtime clock
frequency: https://github.com/aws/aws-fpga/blob/master/hdk/docs/dynamic_clock_config.md[]

Verify that it's loaded by checking status:
----
    $ sudo fpga-describe-local-image -S 0 -R -H -M
----
Note: `-R` forces the PCI bus to refresh AFI Vendor and Device ID, and
`-M` describes the clock setting.

// SUBSECTION ================================================================
=== One-time step: Remove XOCL driver, install XDMA driver

On AWS F1, the host-side talks to the hardware over a PCI bus, using
DMA, which terminates at the `DMA_PCIS` Verilog port connected to your
design.

Details about this step are in:
https://github.com/aws/aws-fpga/blob/master/sdk/linux_kernel_drivers/xdma/xdma_install.md[].

WARNING:: the following steps worked when attempted on the Amazon
Machine Instance "FPGA Developers AMI", which runs CentOS.  We
recommend staying with this.
+

It did not work when I tried it on an Ubuntu 18.04 AMI.  The web page
cited above says do the following in Ubuntu instead of the
yum-installs described in Step 6a_1 below:
----
    $ sudo apt-get install make
    $ sudo apt-get install gcc
----
But there were complaints about a function `mmiowb()` (which has
recently been removed from the Linux kernel), number of arguments to
some macro (which has recently changed in the Linux kernel), etc..

*Step 6a_1:* Do some installs using the `yum` package manager (on Centos):
----
    $ sudo yum groupinstall "Development tools"
    $ sudo yum install kernel kernel-devel
----

Reboot the kernel, if necessary: The `yum install kernel` above will
say, in its messages, which version of the kernel it installed. Typing
`uname -a` will tell you which version of the kernel you're running.
These two should be the same.  If not, reboot your instance.  This
will disconnect your ssh connection (although your instance will
continuously show as "`running`" in the AWS dashboard); reconnect
after a minute or so, when it allows you to reconnect.

*Step 6a_2:* Reboot
----
    $ sudo shutdown -r now
----

Build the XDMA driver (see caveat above about FPGA Developers AMI/CentOS vs Ubuntu):

*Step 6a_3:* Build XDMA driver:
----
    $ cd  $(AWS_FPGA_REPO_DIR)/sdk/linux_kernel_drivers/xdma
    $ make
----

The HDK `README` says the install of the XDMA driver (used by
host-side software) may fail on Development AMI versions 1.5.x or
later, which come with a preinstalled Xilinx Runtime Environment
(XRT), which contains a pre-installed XOCL driver. This prevents
installation of the XDMA driver. Please first remove the XOCL driver
module.

*Step 6b_0:* check if XOCL is running:
----
    $ lsmod | grep xocl
----
If XOCL is present, it should in the listing.

*Step 6b_1:* remove XOCL driver, if it was running:
----
    $ sudo rmmod xocl
----

Install the XDMA driver (will fail if XOCL driver is still in the kernel).

*Step 6c:* Install XDMA driver:
----
    $ sudo make install
    $ sudo modprobe xdma
----
and verify that it is present:
----
    $ lsmod | grep xdma
----
You should see a line like:
----
    xdma                   72503  0
----

// SUBSECTION ================================================================
=== Build your host-side software

*Step 6d:* Build your host-side software:
----
    $ cd  $(CL_DIR)/software/runtime/
    $ make all
----

// SUBSECTION ================================================================
=== Run your host-side software which will interact with the FPGA

*Step 6e:* run host-side software:
----
    $ cd  $(CL_DIR)/software/runtime/
    $ sudo ./$(SW_EXE)
----

// ================================================================
